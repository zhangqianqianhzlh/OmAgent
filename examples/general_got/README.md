# Graph of Thoughts (GoT) Example

[Graph of Thoughts (GoT)](https://arxiv.org/pdf/2308.09687) is a novel approach that models the information generated by an LLM as an arbitrary graph, where units of information (“LLM thoughts”) are vertices, and edges correspond to dependencies between these vertices.

We follow the idea of [official implementation](https://github.com/spcl/graph-of-thoughts) of GoT. Specially, we implement three tasks: sort, keyword count, and set intersection from the [official implementation](https://github.com/spcl/graph-of-thoughts).



This example demonstrates how to use the framework for GoT tasks. The example code can be found in the `examples/general_got` directory.



```bash
   cd examples/general_got
```

## Overview

This example implements a got workflow that consists of the following components:

1. **Input Interface**
   - Handles user input containing questions, tasks, and meta information (meta information is only used for predeined tasks)
   - Processes additional options for the workflow

2. **Task Split Task**
  - split the task into multiple sub-tasks

3. **GoT Workflow**
   - Execute the sub-tasks using the task generator, which handles the conduct, aggregation, and refinement of sub-tasks. 
   - Evaluate the sub-task results and select the best N of them

4. **Condlude Task**
   - Verify the completion status of the task, ensuring all sub-tasks have been executed and aggragrated appropriately
   - Conclude the response based on all related information

![GoT Workflow](./docs/got_workflow.png)

## Prerequisites

- Python 3.10+
- Required packages installed (see requirements.txt)
- Access to OpenAI API or compatible endpoint (see configs/llms/gpt.yml)
- Redis server running locally or remotely
- Conductor server running locally or remotely

## Configuration

The container.yaml file is a configuration file that manages dependencies and settings for different components of the system, including Conductor connections, Redis connections, and other service configurations. To set up your configuration:

1. Generate the container.yaml file:
   ```bash
   python compile_container.py
   ```
   This will create a container.yaml file with default settings under `examples/general_got`.


2. Configure your LLM settings in `configs/llms/gpt.yml`:
   - Set your OpenAI API key or compatible endpoint through environment variable or by directly modifying the yml file
   ```bash
   export custom_openai_key="your_openai_api_key"
   export custom_openai_endpoint="your_openai_endpoint"
   export model_id="your_model_id"  # e.g. gpt-4, gpt-3.5-turbo
   ```
   - Configure other model settings like temperature as needed through environment variable or by directly modifying the yml file

3. Update settings in the generated `container.yaml`:
   - Modify Redis connection settings:
     - Set the host, port and credentials for your Redis instance
     - Configure `redis_stream_client` sections
   - Update the Conductor server URL under conductor_config section
   - Adjust any other component settings as needed


## Running the Example

3. Run the Program of Thought (PoT) example:

   For terminal/CLI usage:
   ```bash
   python run_cli.py
   ```

   For programmatic usage:
   ```bash
   python run_programmatic.py
   ```

   For lite enginer usage:
   ```bash
   python run_lite.py
   ```


## Troubleshooting

If you encounter issues:
- Verify Redis is running and accessible
- Check your OpenAI API key is valid
- Ensure all dependencies are installed correctly
- Review logs for any error messages
- **Open an issue on GitHub if you can't find a solution, we will do our best to help you out!**